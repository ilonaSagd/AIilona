# -*- coding: utf-8 -*-
"""Сагдатуллина.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12LIyFa0VJF4EfUuRe0P9wrOdO8Ht7tt3
"""

import io
import json
import numpy as np
import pandas as pd
import random
import re
import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.utils import pad_sequences

from sklearn.model_selection import train_test_split

dataset=pd.read_csv('https://raw.githubusercontent.com/ilonaSagd/AIilona/main/rus-5.csv', error_bad_lines=False, delimiter=';',
	    names=['eng', 'rus', ])

dataset

train_ratio = 0.8
validation_ratio = 0.1
test_ratio = 0.1
dataset_input, dataset_target = dataset['eng'].to_list(), dataset['rus'].to_list()

x_train, x_test, y_train, y_test = train_test_split(dataset_input, dataset_target, test_size=1 - train_ratio)

x_val, x_test, y_val, y_test = train_test_split(x_test, y_test, test_size=test_ratio/(test_ratio + validation_ratio))

len(x_train)

print(x_train[:10])
print(y_train[:10])

"""Мы создаем модель перевода на основе ** слова **, но мы по-прежнему хотим сохранить знаки препинания и рассматривать их как отдельные токены, поэтому мы вставим пробел между любыми соответствующими знаками препинания и символами вокруг них. Таким образом, наш токенизатор (который не будет отфильтровывать знаки препинания) будет выводить знаки препинания в виде отдельных токенов."""

def preprocess_sentence(s):
  s = re.sub(r"([?.!,¿])", r" \1 ", s)
  s = re.sub(r'[" "]+', " ", s)
  s = s.strip()
  return s

# Предварительно обработаем как исходное, так и целевое предложения.
train_preprocessed_input = [preprocess_sentence(s) for s in x_train]
train_preprocessed_target = [preprocess_sentence(s) for s in y_train]

train_preprocessed_input[:20]

"""Мы будем использовать Teacher Forcing с нашей моделью перевода (в частности, с декодером). Добавим теги начала предложения (<sos>) и теги конца предложения (<eos>) в начале и конце каждого целевого предложения соответственно."""

def tag_target_sentences(sentences):
  tagged_sentences = map(lambda s: (' ').join(['<sos>', s, '<eos>']), sentences)
  return list(tagged_sentences)

train_tagged_preprocessed_target = tag_target_sentences(train_preprocessed_target)

train_tagged_preprocessed_target[:3]

"""Далее выделим вводные и целевые предложения, позаботившись о том, чтобы сохранить соответствующую пунктуацию.
https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer

Обратите внимание, что мы также включаем токен вне словаря (<unk>) в инициализацию токенизатора. Во время вывода, если токенизатор встретит слово, которое он не видел во время начальной подгонки обучающих данных, это слово будет заменено на <unk>, и системе перевода нужно будет справиться с этим.
"""

# Токенизатор для вводимых предложений на русском, аналог pymorphy2, natasha
source_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='"#$%&()*+-/:;=@[\\]^_`{|}~\t\n')
source_tokenizer.fit_on_texts(train_preprocessed_input)
source_tokenizer.get_config()

type(source_tokenizer)

source_vocab_size = len(source_tokenizer.word_index) + 1
print(source_vocab_size)

# Токенизатор для вводимых предложений на английском
target_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<unk>', filters='"#$%&()*+-/:;=@[\\]^_`{|}~\t\n')
target_tokenizer.fit_on_texts(train_tagged_preprocessed_target)
target_tokenizer.get_config()

target_vocab_size = len(target_tokenizer.word_index) + 1
print(target_vocab_size)

"""Далее мы векторизуем вводные и целевые предложения"""

train_encoder_inputs = source_tokenizer.texts_to_sequences(train_preprocessed_input)

train_encoder_inputs

print(train_encoder_inputs[:15])
print(source_tokenizer.sequences_to_texts(train_encoder_inputs[:15]))

"""Мы создадим две копии каждого векторизованного ** целевого** предложения, причем вторая копия будет сдвинута на единицу.

Приведенная ниже функция берет набор предложений, векторизует их, затем возвращает по две копии каждого. Первый будет включать в себя все токены, кроме последнего, второй будет включать в себя все токены, кроме первого.
"""

def generate_decoder_inputs_targets(sentences, tokenizer):
  seqs = tokenizer.texts_to_sequences(sentences)
  decoder_inputs = [s[:-1] for s in seqs] # Drop the last token in the sentence.
  decoder_targets = [s[1:] for s in seqs] # Drop the first token in the sentence.

  return decoder_inputs, decoder_targets

train_decoder_inputs, train_decoder_targets = generate_decoder_inputs_targets(train_tagged_preprocessed_target,
                                                                              target_tokenizer)

"""Каждый токен предложения decoder_input будет передан в декодер как * next ** ожидаемый токен, и каждый токен предложения *decoder_target будет использоваться для вычисления потерь по сравнению с фактическим выходом декодера; точно так, как мы рассмотрели на слайдах."""

print(train_decoder_inputs[0:7], train_decoder_targets[0:7])

print(target_tokenizer.sequences_to_texts(train_decoder_inputs[:7]),
      target_tokenizer.sequences_to_texts(train_decoder_targets[:7]))

max_encoding_len = len(max(train_encoder_inputs, key=len))
max_encoding_len

max_decoding_len = len(max(train_decoder_inputs, key=len))
max_decoding_len

padded_train_encoder_inputs = pad_sequences(train_encoder_inputs, max_encoding_len, padding='post', truncating='post')
padded_train_decoder_inputs = pad_sequences(train_decoder_inputs, max_decoding_len, padding='post', truncating='post')
padded_train_decoder_targets = pad_sequences(train_decoder_targets, max_decoding_len, padding='post', truncating='post')

print(padded_train_encoder_inputs[0])
print(padded_train_decoder_inputs[0])
print(padded_train_decoder_targets[0])

"""При преобразовании дополненной последовательности обратно в текст заполнение неизвестных заменяется лексемой, не входящей в словарный запас <unk>."""

target_tokenizer.sequences_to_texts([padded_train_decoder_inputs[5]])

"""Обучающий набор данных теперь готов, и мы можем выполнить те же шаги предварительной обработки, чтобы подготовить проверочный набор данных."""

def process_dataset(input, output):

  # Пометим целевые предложения токенами <sos> и <eos>.
  tagged_preprocessed_output = tag_target_sentences(output)

  # Векторизовать исходные предложения кодировщика.
  encoder_inputs = source_tokenizer.texts_to_sequences(input)

  # Векторизовать и создавать входные данные декодера и целевые предложения.
  decoder_inputs, decoder_targets = generate_decoder_inputs_targets(tagged_preprocessed_output,
                                                                    target_tokenizer)

  # Заполните все коллекции.
  padded_encoder_inputs = pad_sequences(encoder_inputs, max_encoding_len, padding='post', truncating='post')
  padded_decoder_inputs = pad_sequences(decoder_inputs, max_decoding_len, padding='post', truncating='post')
  padded_decoder_targets = pad_sequences(decoder_targets, max_decoding_len, padding='post', truncating='post')

  return padded_encoder_inputs, padded_decoder_inputs, padded_decoder_targets

# Набор данных для проверки (препроцессинг)
padded_val_encoder_inputs, padded_val_decoder_inputs, padded_val_decoder_targets = process_dataset(x_val, y_val)

"""Теперь мы готовы к построению нашей модели перевода. Это параметры, которые мы будем использовать"""

embedding_dim = 128
hidden_dim = 256
default_dropout=0.3
batch_size = 32 # 16, 32
epochs = 14 # 4

# Начальный входной слой кодировщика, который будет принимать дополненные последовательности
# здесь нет shape, но вы можете указать ее заранее
encoder_inputs = layers.Input(shape=[None], name='encoder_inputs')

# Слой эмбеддингов
encoder_embeddings = layers.Embedding(source_vocab_size,
                                      embedding_dim,
                                      mask_zero=True,
                                      name='encoder_embeddings')

# Передача выходных данных входного слоя встраиваемому слою создает связь
# Входные последовательности теперь будут поступать в слой эмбеддингов, который будет выводить
# sequence of embeddings
encoder_embedding_output = encoder_embeddings(encoder_inputs)


# В этой модели мы не используем какой-либо механизм внимания, поэтому устанавливаем только
# return_state к True. return_sequences остается False.
encoder_lstm = layers.LSTM(hidden_dim,
                           return_state=True,
                           dropout=default_dropout,
                           name='encoder_lstm') # LSTM, GRU

# Передача выходных данных встраиваемого слоя на уровень LSTM создает еще одну ссылку.
# ВАЖНО: LSTM всегда возвращает три значения. Когда return_sequences равен
# False, encoder_outputs и state_h совпадают. Когда return_sequences равен
# True, encoder_outputs содержит скрытые состояния кодера с каждого временного шага.
#
# Дополнительное примечание: мы не будем использовать здесь encoder_outputs, чтобы эта переменная могла быть
# заменяется на _
encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding_output)

# начальные состояния для декодера.
encoder_states = (state_h, state_c)

decoder_inputs = layers.Input(shape=[None], name='decoder_inputs')


decoder_embeddings = layers.Embedding(target_vocab_size,
                                      embedding_dim,
                                      mask_zero=True,
                                      name='decoder_embeddings')


decoder_embedding_output = decoder_embeddings(decoder_inputs)

# Return sequences - True.
decoder_lstm = layers.LSTM(hidden_dim,
                           return_sequences=True,
                           return_state=True,
                           dropout=default_dropout,
                           name='decoder_lstm')


# Установите начальное состояние декодера на конечные выходные состояния кодера. С
# return_sequences имеет значение True, decoder_outputs будет представлять собой набор
# скрытого состояния декодера на каждом временном шаге. Также обратите внимание, что поскольку нам не нужно
# конечный скрытый вывод декодера и состояния ячеек, они просто установлены в _.
decoder_outputs, _, _ = decoder_lstm(decoder_embedding_output, initial_state=encoder_states)

# В конце добавьте слой softmax, чтобы создать распределение вероятностей для выходного слова.
decoder_dense = layers.Dense(target_vocab_size, activation='softmax', name='decoder_dense')

# Распределение вероятностей для выходного слова.
y_proba = decoder_dense(decoder_outputs)

"""Теперь мы можем создать нашу модель и указать, что она имеет * два ** входа и один выход (many-to-one).

Обратите внимание, что *точность является приблизительным показателем для моделей перевода и просто служит показателем того, как работает наша модель. Точность оценим показателем BLEU.
"""

# Обратите внимание, как модель принимает два входных сигнала в массиве.
model = tf.keras.Model([encoder_inputs, decoder_inputs], y_proba, name='rus_eng_seq2seq_nmt_no_attention')

model.compile(optimizer='adam', loss='sparse_categorical_crossentropy',  metrics='sparse_categorical_accuracy')
model.summary()

"""Мы можем визуализировать нашу модель, чтобы получить лучшее представление о построенном нами потоке."""

from keras.utils import plot_model

plot_model(model, to_file='rus_eng_seq2seq_nmt_no_attention.png', show_shapes=True, show_layer_names=True)

"""Итак, вот измерения, с которыми мы работаем в этой модели обучения."""

print('encoder_inputs layer\n input dimension {}\n output dimension: {}'.format((batch_size, max_encoding_len), (batch_size, max_encoding_len)))
print()
print('encoder_embeddings layer\n input dimension {}\n output dimension: {}'.format((batch_size, max_encoding_len), (batch_size, max_encoding_len, embedding_dim)))
print()
print('encoder_lstm layer\n input dimension {}\n output dimension: {}'.format((batch_size, max_encoding_len, embedding_dim), [(batch_size, hidden_dim), (batch_size, hidden_dim), (batch_size, hidden_dim)]))
print()
print()
print('decoder_inputs layer\n input dimension {}\n output dimension: {}'.format((batch_size, max_decoding_len), (batch_size, max_decoding_len)))
print()
print('decoder_embeddings layer\n input dimension {}\n output dimension: {}'.format((batch_size, max_decoding_len), (batch_size, max_decoding_len, embedding_dim)))
print()
print('decoder_lstm layer\n input dimension {}\n output dimension: {}'.format([(batch_size, max_decoding_len, embedding_dim), (batch_size, hidden_dim), (batch_size, hidden_dim)], [(batch_size, max_decoding_len, hidden_dim), (batch_size, hidden_dim), (batch_size, hidden_dim)]))
print()
print('decoder_dense layer(softmax)\n input dimension {}\n output dimension: {}'.format((batch_size, max_decoding_len, hidden_dim), (batch_size, max_decoding_len, target_vocab_size)))

# Сохраним модель
filepath="./RusEngNMTNoAttention/training1/cp.ckpt"

# Создим обратный вызов, который сохранит веса модели
cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=filepath,
                                                 save_weights_only=True,
                                                 verbose=1)

es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)

history = model.fit([padded_train_encoder_inputs, padded_train_decoder_inputs], padded_train_decoder_targets,
                     batch_size=batch_size,
                     epochs=epochs,
                     validation_data=([padded_val_encoder_inputs, padded_val_decoder_inputs], padded_val_decoder_targets),
                     callbacks=[cp_callback, es_callback])

##### Сохраните модель.
model.save('ru_eng_s2s_nmt_no_attention')


##### Заархивируйте и загрузите модель.
!zip -r ./rus_eng_s2s_nmt_no_attention.zip ./rus_eng_s2s_nmt_no_attention
# files.download("./rus_eng_s2s_nmt_no_attention.zip") # Скачать из гугл colab


##### Сохраните токенизаторы в виде файлов JSON. Полученные файлы можно загрузить, щелкнув по ним левой кнопкой мыши
source_tokenizer_json = source_tokenizer.to_json()
with io.open('source_tokenizer.json', 'w', encoding='utf-8') as f:
  f.write(json.dumps(source_tokenizer_json, ensure_ascii=False))

target_tokenizer_json = target_tokenizer.to_json()
with io.open('target_tokenizer.json', 'w', encoding='utf-8') as f:
  f.write(json.dumps(target_tokenizer_json, ensure_ascii=False))

# здесь загружаем тестовые данные и сохраненную модель
!unzip -o rus_eng_s2s_nmt_no_attention_tokenizers.zip

with open('source_tokenizer.json') as f:
    data = json.load(f)
    source_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)

with open('target_tokenizer.json') as f:
    data = json.load(f)
    target_tokenizer = tf.keras.preprocessing.text.tokenizer_from_json(data)

!unzip -o rus_eng_s2s_nmt_no_attention.zip

# Загрузить модель в программу
model = tf.keras.models.load_model('rus_eng_s2s_nmt_no_attention')

x_test[:3]

y_test[:3]

# Препроцессинг тестового датасета
padded_test_encoder_inputs, padded_test_decoder_inputs, padded_test_decoder_targets = process_dataset(x_test, y_test)

model.save("rus_translator.h5")

# Оценить модель на тестовом наборе.
model.evaluate([padded_test_encoder_inputs, padded_test_decoder_inputs], padded_test_decoder_targets)

"""Наша модель обучения использует teacher forcing. Создадим ** отдельные**, автономные модели энкодера и декодера"""

# Это слои обученной модели.
[layer.name for layer in model.layers]

encoder_inputs = model.get_layer('encoder_inputs').input

encoder_embedding_layer = model.get_layer('encoder_embeddings')
encoder_embeddings = encoder_embedding_layer(encoder_inputs)

encoder_lstm = model.get_layer('encoder_lstm')

_, encoder_state_h, encoder_state_c = encoder_lstm(encoder_embeddings)

encoder_states = [encoder_state_h, encoder_state_c]

# Our stand-alone encoder model. encoder_inputs is the input to the encoder,
# and encoder_states is the expected output.
encoder_model_no_attention = tf.keras.Model(encoder_inputs, encoder_states)

plot_model(encoder_model_no_attention, to_file='encoder_model_no_attention_plot.png', show_shapes=True, show_layer_names=True)

decoder_inputs = model.get_layer('decoder_inputs').input

decoder_embedding_layer = model.get_layer('decoder_embeddings')
decoder_embeddings = decoder_embedding_layer(decoder_inputs)

# Входные данные для представления скрытых состояний LSTM декодера и состояний ячеек. Мы заселим
# это делается вручную, используя выходные данные кодировщика для исходного состояния.
decoder_input_state_h = tf.keras.Input(shape=(hidden_dim,), name='decoder_input_state_h')
decoder_input_state_c = tf.keras.Input(shape=(hidden_dim,), name='decoder_input_state_c')
decoder_input_states = [decoder_input_state_h, decoder_input_state_c]

decoder_lstm = model.get_layer('decoder_lstm')

decoder_sequence_outputs, decoder_output_state_h, decoder_output_state_c = decoder_lstm(
    decoder_embeddings, initial_state=decoder_input_states
)

# Обновите скрытые состояния и состояния ячеек для следующего временного шага.
decoder_output_states = [decoder_output_state_h, decoder_output_state_c]

decoder_dense = model.get_layer('decoder_dense')
y_proba = decoder_dense(decoder_sequence_outputs)

decoder_model_no_attention = tf.keras.Model(
    [decoder_inputs] + decoder_input_states,
    [y_proba] + decoder_output_states
)

plot_model(decoder_model_no_attention, to_file='decoder_model_no_attention_plot.png', show_shapes=True, show_layer_names=True)

def translate_without_attention(sentence: str,
                                source_tokenizer, encoder,
                                target_tokenizer, decoder,
                                max_translated_len = 30):

  # Векторизовать исходное предложение и прогнать его через кодировщик.
  input_seq = source_tokenizer.texts_to_sequences([sentence])

  #  Получите маркированное предложение, чтобы увидеть, есть ли какие-либо неизвестные маркеры.
  tokenized_sentence = source_tokenizer.sequences_to_texts(input_seq)

  states = encoder.predict(input_seq)

  current_word = '<sos>'
  decoded_sentence = []

  while len(decoded_sentence) < max_translated_len:

    # Set the next input word for the decoder.
    target_seq = np.zeros((1,1))
    target_seq[0, 0] = target_tokenizer.word_index[current_word]

    # Determine the next word.
    target_y_proba, h, c = decoder.predict([target_seq] + states)
    target_token_index = np.argmax(target_y_proba[0, -1, :])
    current_word = target_tokenizer.index_word[target_token_index]

    if (current_word == '<eos>'):
      break

    decoded_sentence.append(current_word)
    states = [h, c]

  return tokenized_sentence[0], ' '.join(decoded_sentence)

"""Чтобы протестировать это, мы выберем несколько предложений из набора данных test и переведем их."""

# random.seed здесь только для того, чтобы воссоздать результаты.
random.seed(1)
sentences = random.sample(x_test, 15)
sentences

def translate_sentences(source, target, translation_func, source_tokenizer, encoder,
                        target_tokenizer, decoder):
    translations = {'Tokenized Original': [], 'Reference': [], 'Translation': []}

    count = len(source)
    text_index = 0
    for sentence in source:
        print(text_index)
        source_text, target_text = sentence, target[text_index]
        print(source_text)
        print(target_text)
        source = preprocess_sentence(source_text)
        tokenized_sentence, translated = translation_func(source_text, source_tokenizer, encoder,
                                                          target_tokenizer, decoder)

        translations['Tokenized Original'].append(tokenized_sentence)
        translations['Reference'].append(target_text)
        translations['Translation'].append(translated)
        text_index += 1
    return translations

translations_no_attention = pd.DataFrame(translate_sentences(x_test[:10], y_test[:10], translate_without_attention,
                                                             source_tokenizer, encoder_model_no_attention,
                                                             target_tokenizer, decoder_model_no_attention))
translations_no_attention[:10]

"""Таким образом, производительность здесь, как правило, смешана с несколькими ошибочными переводами, несколькими, которые точно соответствуют ссылке, и несколькими интересными, где перевод не соответствует ссылке, но передает ту же идею. Но в целом, довольно неплохо.

Реккурентный машинный перевод Seq2Seq С Luong Attention
"""

class Encoder(tf.keras.Model):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(Encoder, self).__init__()

        # No masking here. We'll handle it ourselves.
        self.embedding = layers.Embedding(source_vocab_size,
                                          embedding_dim,
                                          name='encoder_embedding_layer')

        # return_sequences is set to True this time.
        self.lstm = layers.LSTM(hidden_dim,
                                return_sequences=True,
                                return_state=True,
                                name='encoder_lstm')

    def call(self, input):
        embeddings = self.embedding(input)

        # output_seq will hold the encoder's hidden states from each time step.
        output_seq, state_h, state_c = self.lstm(embeddings)

        return output_seq, state_h, state_c

"""Мы можем получить представление о выходных данных кодировщика, создав экземпляр фиктивного кодировщика и передав ему некоторые входные данные."""

test_encoder = Encoder(source_vocab_size, embedding_dim, hidden_dim)

test_encoder_batch = padded_train_encoder_inputs[:3]
print(test_encoder_batch.shape)
test_encoder_batch

test_encoder_outputs, state_h, state_c = test_encoder(test_encoder_batch)

print(test_encoder_outputs.shape)
print(state_h.shape)
print(state_c.shape)

# Выборочный вывод кодера LSTM для одиночной последовательности длиной 4.
encoder_out = tf.constant([[1., 2., 3.],
                           [2., 3., 4.],
                           [3., 4., 5.],
                           [4., 5. ,6.]])

print('encoder_out shape: {}'.format(encoder_out.shape))
print('Number of timesteps: {}'.format(encoder_out.shape[0]))
print('Number of hidden dimensions: {}'.format(encoder_out.shape[1]))

"""И предположим, что следующее - это скрытое состояние от декодера на определенном временном шаге."""

# Выборка выходных данных LSTM декодера для одного временного шага.
decoder_out = tf.constant([[1., 3., 5.]])

print('decoder_out shape: {}'.format(decoder_out.shape))
print('Number of timesteps: {}'.format(decoder_out.shape[0]))
print('Number of hidden dimensions: {}'.format(decoder_out.shape[1]))

"""Чтобы получить оценки внимания, нам нужно выполнить точечное произведение между скрытым состоянием декодера и скрытыми состояниями кодера. Чтобы сделать это, необходимо транспонировать скрытые состояния кодера."""

tf.transpose(encoder_out)

attention_scores = tf.matmul(decoder_out, encoder_out, transpose_b=True)
print(attention_scores)

attention_weights = tf.keras.activations.softmax(attention_scores, axis=-1)
print(attention_weights)

context = tf.matmul(attention_weights, encoder_out)
print(context)

class LuongAttention(tf.keras.Model):
  def __init__(self, hidden_dim):
    super(LuongAttention, self).__init__()

    self.w = layers.Dense(hidden_dim, name='encoder_outputs_dense')

  def call(self, inputs):
    encoder_output_seq, decoder_output = inputs
    z = self.w(encoder_output_seq)
    attention_scores = tf.matmul(decoder_output, z, transpose_b=True)
    attention_weights = tf.keras.activations.softmax(attention_scores, axis=-1)
    context = tf.matmul(attention_weights, encoder_output_seq)

    return attention_weights, context

"""Декодер остается в основном тем же самым, за исключением:

Слой встраивания не имеет маскировки.
В потоке есть дополнительный шаг, требующий внимания.
Объединенный выходной вектор контекста и декодера проходит через плотный слой w. Обратитесь к видео / слайдам этого модуля, если потребуется освежить информацию.
Последний плотный слой не имеет активации softmax. Скорее всего, мы рассчитаем потери непосредственно по логитам.
В качестве входных данных декодер принимает:

токен(ы) для текущего шага. Во время обучения с принуждением учителя это будет следующий ожидаемый знак (знаки). Во время вывода это будут токены, сгенерированные на последнем временном шаге.
все скрытые состояния кодировщика.
независимо от того, что декодер должен принимать в качестве своих текущих скрытых состояний и состояний ячейки.
Для выходных данных декодер возвращает:

логиты.
последнее скрытое значение LSTM и состояния ячейки.
взвешивание внимания.
"""

class Decoder(tf.keras.Model):
  def __init__(self, vocab_size, embedding_dim, hidden_dim):
    super(Decoder, self).__init__()

    self.embedding_layer = layers.Embedding(vocab_size,
                                            embedding_dim,
                                            name='decoder_embedding_layer')

    self.lstm = layers.LSTM(hidden_dim,
                            return_sequences=True,
                            return_state=True,
                            name='decoder_lstm')

    self.attention = LuongAttention(hidden_dim)

    self.w = tf.keras.layers.Dense(hidden_dim, activation='tanh', name='attended_outputs_dense')

    self.dense = layers.Dense(vocab_size, name='decoder_dense')


  def call(self, inputs):
    decoder_input, encoder_output_seq, lstm_state = inputs
    embeddings = self.embedding_layer(decoder_input)

    decoder_output, state_h, state_c = self.lstm(embeddings, initial_state=lstm_state)

    weights, context = self.attention([encoder_output_seq, decoder_output])

    decoder_output_with_attention = self.w(tf.concat(

                                                     [tf.squeeze(context, 1), tf.squeeze(decoder_output, 1)], -1))

    logits = self.dense(decoder_output_with_attention)

    return logits, state_h, state_c, weights

"""Мы можем получить представление о входах и выходах декодера за один шаг обучения с помощью теста."""

test_decoder = Decoder(target_vocab_size, embedding_dim, hidden_dim)

"""Предположим, что это пакет входных последовательностей для декодера (последовательности, используемые для teacher forcing)..."""

test_decoder_batch = padded_train_decoder_inputs[:3]
print(test_decoder_batch.shape)
test_decoder_batch

"""......и давайте предположим, что в данный момент мы находимся на втором временном шаге (index 1).Это будут следующие входные данные для декодера для каждой последовательности в пакете:"""

test_decoder_batch[:, 1]

"""Но нам нужно изменить это так, чтобы это были три последовательности из одного элемента, что мы можем сделать с помощью expand_dims."""

next_decoder_inputs = tf.expand_dims(test_decoder_batch[:, 1], 1)
next_decoder_inputs

# Начальные значения для state_h и state_care из декодировщика.
test_decoder_logits, state_h, state_c, test_decoder_weights = test_decoder(
    [
      next_decoder_inputs,
      test_encoder_outputs,
      [state_h, state_c]
    ])

print(test_decoder_logits.shape)
print(test_decoder_weights.shape)

"""Мы собираемся использовать пользовательскую функцию потерь, потому что наши последовательности содержат заполнение, и мы не хотим, чтобы прогнозы по заполнению приводили к потерям."""

def loss_func(targets, logits):
  ce_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)

  mask = tf.cast(tf.math.not_equal(targets, 0), tf.float32)

  return ce_loss(targets, logits, sample_weight=mask)

dataset = tf.data.Dataset.from_tensor_slices((padded_train_encoder_inputs,
                                              padded_train_decoder_inputs,
                                              padded_train_decoder_targets)).batch(batch_size, drop_remainder=True)

class TranslatorTrainer(tf.keras.Model):
  def __init__(self, encoder, decoder):
    super(TranslatorTrainer, self).__init__()

    self.encoder = encoder
    self.decoder = decoder

  # This method will be called by model.fit for each batch.
  @tf.function
  def train_step(self, inputs):
      loss = 0.

      encoder_input_seq, decoder_input_seq, decoder_target_seq = inputs

      with tf.GradientTape() as tape:
          encoder_output_seq, state_h, state_c = self.encoder(encoder_input_seq)

          # We need to create a loop to iterate through the target sequences
          for i in range(decoder_target_seq.shape[1]):

              # Input to the decoder must have shape of (batch_size, length)
              # so we need to expand one dimension (just like in the previous example).
              next_decoder_input = tf.expand_dims(decoder_input_seq[:, i], 1)
              logits, state_h, state_c, _ = self.decoder(
                  [next_decoder_input, encoder_output_seq, (state_h, state_c)])

              # The loss is now accumulated through the whole batch
              loss += self.loss(decoder_target_seq[:, i], logits)

      # Update the parameters and the optimizer
      variables = encoder.trainable_variables + decoder.trainable_variables
      gradients = tape.gradient(loss, variables)
      self.optimizer.apply_gradients(zip(gradients, variables))

      return {'loss': loss / decoder_target_seq.shape[1]}

encoder = Encoder(source_vocab_size, embedding_dim, hidden_dim)
decoder = Decoder(target_vocab_size, embedding_dim, hidden_dim)
optimizer = tf.keras.optimizers.Adam()

translator_trainer = TranslatorTrainer(encoder, decoder)
translator_trainer.compile(optimizer=optimizer, loss=loss_func)

epochs = 12
translator_trainer.fit(dataset, epochs=epochs)

"""Функции для сохранения, архивирования и загрузки весов обученных кодировщика и декодера."""

encoder.save_weights('attention_encoder_weights_with_dropout_ckpt')
decoder.save_weights('attention_decoder_weights_with_dropout_ckpt')

!zip -r ./attention_weights.zip ./attention_weights

# files.download('./attention_weights.zip')

# Загрузить обученную модель
# !wget
# !unzip -o attention_weights.zip

# encoder.load_weights('attention_weights/attention_encoder_weights_ckpt')
# decoder.load_weights('attention_weights/attention_decoder_weights_ckpt')

def translate_with_attention(sentence: str,
                             source_tokenizer, encoder,
                             target_tokenizer, decoder,
                             max_translated_len = 30):
    input_seq = source_tokenizer.texts_to_sequences([sentence])
    tokenized = source_tokenizer.sequences_to_texts(input_seq)

    input_seq = pad_sequences(input_seq, maxlen=max_encoding_len, padding='post')
    encoder_output, state_h, state_c  = encoder.predict(input_seq)

    current_word = '<sos>'
    decoded_sentence = []

    while len(decoded_sentence) < max_translated_len:
        target_seq = np.zeros((1,1))
        target_seq[0, 0] = target_tokenizer.word_index[current_word]

        logits, state_h, state_c, _ = decoder.predict([target_seq, encoder_output, (state_h, state_c)])
        current_token_index = np.argmax(logits[0])

        current_word = target_tokenizer.index_word[current_token_index]

        if (current_word == '<eos>'):
          break

        decoded_sentence.append(current_word)

    return tokenized[0], ' '.join(decoded_sentence)

"""Мы переведем те же самые случайно выбранные предложения, что и раньше, и сравним результаты с переводами, не обращая внимания."""

shorter_translations_w_attention = pd.DataFrame(translate_sentences(x_test[:10], y_test[:10],
                                                                    translate_with_attention,
                                                                    source_tokenizer,
                                                                    encoder,
                                                                    target_tokenizer,
                                                                    decoder=decoder))

shorter_translations_w_attention.rename(columns={'Translation': 'Translation W/ Attention'}, inplace=True)
shorter_translations_w_attention['Translation W/O Attention'] = translations_no_attention['Translation']
shorter_translations_w_attention

"""Кроме того, давайте взглянем на результаты перевода самых длинных предложений в обучающем наборе, чтобы увидеть, насколько они хороши."""

longer_data = pd.DataFrame({'source': x_train, 'target': y_train})

sorted_data = longer_data.sort_values(by="source", key=lambda s: s.str.len()).reset_index(drop=True).copy()

longer_sentences = sorted_data[-10:]
x_longerData = longer_sentences['source']
y_longerData = longer_sentences['target']

y_longerData.to_list()

longer_translations_with_attention = pd.DataFrame(translate_sentences(x_longerData.to_list(), y_longerData.to_list(), translate_with_attention,
                                                                      source_tokenizer, encoder,
                                                                      target_tokenizer, decoder))
longer_translations_with_attention.rename(columns={'Translation': 'Translation W/ Attention'}, inplace=True)
longer_translations_with_attention['Translation W/O Attention'] = longer_translations_wo_attention['Translation']
longer_translations_with_attention

"""Итак, очевидно, что кодер / декодер с вниманием продвигается дальше, прежде чем потерять сигнал, и ему часто удается перевести слова позже в предложении."""